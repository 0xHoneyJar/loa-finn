---
title: loa-finn — Capability Brief
version: 1.0.0
---

# loa-finn — Capability Brief

> Generated by Ground Truth v1.0.0
> Grounded in: features.yaml + codebase analysis


## Overview

<!-- provenance: REPO-DOC-GROUNDED -->
loa-finn is a persistent autonomous agent server that manages durable state, multi-model orchestration, code review automation, and scheduled job execution. See `grimoires/loa/prd-ground-truth.md §1` for the full problem statement.

<!-- provenance: ANALOGY -->
The architecture follows a layered composition pattern similar to how Kubernetes separates its control plane (scheduling, routing, health) from its data plane (actual workload execution). Each capability area operates independently but communicates through well-defined interfaces.


## Durable State Management

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=WALManager, symbol=createWALManager, symbol=GitSync, symbol=ObjectStoreSync, symbol=WALPruner -->
The persistence layer is a barrel export at `src/persistence/index.ts:1-30` that composes multiple durability strategies. The `WALManager` and `createWALManager` factory handle append-only writes with crash recovery, while `GitSync`, `ObjectStoreSync`, and `WALPruner` provide complementary sync mechanisms for git snapshots, R2 object storage, and segment compaction respectively. Together they form a recovery cascade — state can be restored from any source.

<!-- provenance: ANALOGY -->
This is the same pattern PostgreSQL uses for its write-ahead log — append-only writes ensure no partial pages reach disk, making crash recovery deterministic rather than hopeful. The multi-source sync mirrors how Bigtable uses both local SSTable files and remote GFS for durability.

> **Known limitation**: Single-writer only; no concurrent sessions per WAL file — design choice for consistent state during compound learning.

> **Known limitation**: S3 compatibility untested with non-Cloudflare providers — only Cloudflare R2 used in production.


## Multi-Model Orchestration

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=HounfourRouterOptions, symbol=ProviderRegistry, symbol=BudgetEnforcer -->
The routing layer at `src/hounfour/router.ts:38-50` defines `HounfourRouterOptions` — the composition root for model orchestration. It accepts a `ProviderRegistry`, `BudgetEnforcer`, health prober, rate limiter, optional pool registry for tier-based routing, and optional BYOK proxy adapter. This single interface wires all orchestration concerns. Budget enforcement uses scope-key derivation to compute canonical keys for project, phase, and sprint scopes, ensuring all budget operations use consistent addressing.

<!-- provenance: ANALOGY -->
The pool registry and routing system functions like Kubernetes Service Discovery — pool IDs serve as service names, and the router resolves them to concrete provider endpoints with health-aware failover. The circuit breaker implementation mirrors Netflix Hystrix's three-state machine (closed, open, half-open).

> **Known limitation**: IDX Ensemble strategy is experimental; no Gemini adapter yet — adapter pattern supports future providers.


## Autonomous Code Review

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=loadFinnConfig, symbol=createFinnAdapters, symbol=SanitizedLogger -->
The BridgeBuilder review pipeline at `src/bridgebuilder/entry.ts:1-10` composes a multi-step pipeline: configuration loading via `loadFinnConfig`, adapter creation through `createFinnAdapters`, and sanitized logging with `SanitizedLogger`. The pipeline processes pull requests through structured review dimensions.

<!-- provenance: ANALOGY -->
Like Stripe's documentation-first approach, BridgeBuilder generates review comments that explain mechanisms rather than assert quality. Each review comment is grounded in specific code evidence, letting developers form their own conclusions.

> **Known limitation**: Review bot can only COMMENT on PRs, cannot APPROVE or REQUEST_CHANGES — safety constraint requiring human approval for merge gates.


## Compound Learning

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=TrajectoryEntry, symbol=CandidateLearning, symbol=LearningStore -->
The learning system at `src/learning/compound.ts:1-29` captures session trajectories via `TrajectoryEntry` records (message, tool_start, tool_end events) and extracts `CandidateLearning` objects with trigger, context, resolution, and confidence scoring. It uses the upstream `LearningStore` for persistence and WALManager for trajectory logging.

<!-- provenance: ANALOGY -->
This follows the same feedback loop pattern TensorFlow uses for model improvement — capture execution traces, extract patterns that generalize, and apply quality gates before promoting discoveries to durable storage.

> **Known limitation**: Learning extraction requires LLM call; adds latency to session close — quality gates for candidates require reasoning.


## Job Scheduling & Lifecycle

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=CronService, symbol=CircuitBreaker, symbol=computeNextRunAtMs -->
The scheduling system at `src/cron/service.ts:1-8` provides `CronService` with tick-based job execution, stuck-job detection (default 2-hour timeout), and per-job `CircuitBreaker` isolation. Next-run computation via `computeNextRunAtMs` handles cron expression parsing.

<!-- provenance: ANALOGY -->
The job scheduling framework mirrors Apache Airflow's DAG scheduler — declarative job registration with dependency-aware execution, circuit breakers per task, and idempotency guards preventing duplicate runs on retry.


## Safety & Audit Infrastructure

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=PoolErrorCode, symbol=PoolError -->
The worker pool at `src/agent/worker-pool.ts:17-30` implements priority-lane execution with typed error codes. `PoolErrorCode` defines five failure modes: WORKER_UNAVAILABLE (retry with backoff), POOL_SHUTTING_DOWN (do not retry), SANDBOX_DISABLED, EXEC_TIMEOUT (retry once), and WORKER_CRASHED (auto-recovered by pool). The `PoolError` class wraps each code for structured error handling.

<!-- provenance: ANALOGY -->
The sandbox execution model follows the same isolation pattern as Chrome's multi-process architecture — each tool invocation runs in a separate worker thread with enforced timeouts and resource limits, preventing a single runaway operation from affecting the host.

> **Known limitation**: Worker thread sandbox has 30s default timeout; long-running tools may be killed — safety constraint preventing resource exhaustion.


## HTTP Gateway & Dashboard

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=createApp, symbol=AppOptions -->
The gateway at `src/gateway/server.ts:19-30` uses Hono as its HTTP framework. The `createApp` function accepts `AppOptions` with optional dependencies: HealthAggregator, ActivityFeed, SandboxExecutor, WorkerPool, HounfourRouter, and S2SJwtSigner for JWT-authenticated endpoints.

<!-- provenance: ANALOGY -->
The gateway composition mirrors how Vercel's edge runtime wires middleware — authentication, rate limiting, CORS, and routing are composed as independent middleware layers, each injectable and testable in isolation.

> **Known limitation**: No horizontal scaling — single Hono instance per deployment. Design choice simplifying state management.

> **Known limitation**: Activity feed relies on GitHub API rate limits (5000 req/hr authenticated) — no local caching beyond in-memory TTL.


## Design Principles

<!-- provenance: REPO-DOC-GROUNDED -->
The architecture follows three core principles documented in `grimoires/loa/sdd-ground-truth.md §3`: deterministic verification (shell scripts only in the verification path), human-curated registries (features.yaml is never auto-generated), and fail-fast validation (PATH_SAFETY runs before any file I/O).

<!-- provenance: ANALOGY -->
These principles reflect the same philosophy behind HashiCorp's Sentinel policy framework — automated enforcement of human-authored rules, where the policy engine is deterministic but the policies themselves encode human judgment about what constitutes acceptable behavior.


## Limitations & Honest Assessment

<!-- provenance: CODE-FACTUAL -->
<!-- evidence: symbol=WALManager -->
The WAL persistence layer at `src/persistence/index.ts:5` currently supports single-writer access only. This is a deliberate design constraint — compound learning requires consistent state within a session, and multi-writer coordination would add complexity without clear benefit at current scale.

<!-- provenance: HYPOTHESIS -->
We hypothesize that horizontal scaling will require a distributed WAL implementation (perhaps using CRDTs or Raft consensus) if the system needs to support more than one concurrent agent session per deployment. This remains a research-stage consideration.


## What This Means

<!-- provenance: ANALOGY -->
loa-finn combines patterns proven at scale — PostgreSQL's crash-safe logging, Kubernetes' service discovery, Netflix's circuit breakers, and Stripe's documentation-first philosophy — into a cohesive autonomous agent server. The mechanism descriptions in this document are designed to let developers form their own conclusions through evidence rather than adjectives. Every claim is grounded in a specific file and line range, verified by deterministic shell scripts that run without LLM involvement.

<!-- ground-truth-meta: head_sha=c38e15dccc03def2570c48fb1215dea3bdb866c7 generated_at=2026-02-10T09:41:10Z features_sha=9cf7d7638dc91671ccb4b2c72bd1bcd2aa0d2dfb limitations_sha=e5c7ffff6e5bdbff142b490a20f612f9165aeec8 ride_sha=none -->
